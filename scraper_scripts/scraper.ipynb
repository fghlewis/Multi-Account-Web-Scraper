{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraper\n",
    "Here lies the core component of the project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright, Browser, Page\n",
    "from get_credentials import get_account\n",
    "import sqlite3\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a new browser context with specified settings``\n",
    "async def create_context(p, proxy=None):\n",
    "\n",
    "    # we launch our browser, we use Browser which is a type hint for the browser object and pass in some settings\n",
    "    browser: Browser = await p.chromium.launch(\n",
    "        # our proxy settings, if a proxy is provided\n",
    "            proxy={\"server\": f\"http://{proxy}\"} if proxy else None,\n",
    "            # we can set the browser to be headless or not, for now we set it to False so we can see the browser in action\n",
    "            headless=False,\n",
    "            # we set the slow_mo to a random value between 100 and 300 milliseconds to slow down each browser action\n",
    "            slow_mo=random.randint(100, 300)\n",
    "        )  # Set to True later for performance\n",
    "\n",
    "# create a new browser context with specific settings\n",
    "    context = await browser.new_context(\n",
    "        # set the viewport size to 1280x720 for our browser context\n",
    "            viewport={\"width\": 1280, \"height\": 720},\n",
    "            # pass in the user agent string to mimic a real browser\n",
    "            user_agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36\",\n",
    "            # set the locale to English (United States)\n",
    "            locale=\"en-US\",\n",
    "            # optional: add extra HTTP headers if needed for more realistic requests (I don't use them here)\n",
    "            extra_http_headers={}\n",
    "        )\n",
    "    return browser, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our main scraping function\n",
    "async def scrape_page(p, url, username, password, proxy, account_id=None, status=None):\n",
    "\n",
    "# get out browser and context from our previously defined function\n",
    "    browser, context = await create_context(p, proxy)\n",
    "\n",
    "# again we use type hinting to specify that page is of type Page and create a new page in our context\n",
    "    page: Page = await context.new_page()\n",
    "\n",
    "    # print statement to show which proxy is being used\n",
    "    print(f\"Scraping with Proxy: {proxy}\")\n",
    "\n",
    "    # set default value for status\n",
    "    status = \"active\"\n",
    "\n",
    "# wrap our scraping logic in a try-except block to handle any errors that may occur\n",
    "    try:\n",
    "        # Step 1: Navigate to the login page and wait for it to load\n",
    "        await page.goto(url, wait_until=\"domcontentloaded\", timeout=60000)\n",
    "        \n",
    "            # Fill in the login form\n",
    "        await page.fill('input[name=\"username\"]', username)\n",
    "        await page.fill('input[name=\"password\"]', str(password))\n",
    "\n",
    "        # Click the login button\n",
    "        await page.click('input[type=\"submit\"]')\n",
    "\n",
    "        # Wait for div.qoute to appear\n",
    "        await page.wait_for_selector(\"div.quote\")\n",
    "        \n",
    "        # scrape the page content and save it to soup variable\n",
    "        content = await page.content()\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "        # Step 3: Parse all quote blocks\n",
    "        quotes = soup.select(\"div.quote\")\n",
    "        random_quote = random.choice(quotes)\n",
    "\n",
    "        # save the quote text and author to variables\n",
    "        text = random_quote.select_one(\"span.text\").get_text(strip=True)\n",
    "        author = random_quote.select_one(\"small.author\").get_text(strip=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # if any error occurs, we print the error message and set text, author, and status to \"N/A\" or \"error\"\n",
    "        print(f\"[!] Error: {e}\")\n",
    "        text = \"N/A\"\n",
    "        author = \"N/A\"\n",
    "        status = \"error\"\n",
    "    \n",
    "    finally:\n",
    "        # Insert into the database\n",
    "        conn = sqlite3.connect(\"../database/scraper.db\")\n",
    "        conn.execute(\"PRAGMA foreign_keys = ON\")\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "    # udate the quotes table with the scraped data\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO quotes (quote, author, scraped_at, status, account_id)\n",
    "            VALUES (?, ?, ?, ?, ?)\n",
    "        \"\"\", (text, author, datetime.now(), status, account_id))\n",
    "\n",
    "        # if account_id is provided, update the last_used timestamp in the accounts table\n",
    "        cursor.execute(\"\"\"\n",
    "            UPDATE accounts\n",
    "            SET last_used = ?\n",
    "            WHERE id = ?\n",
    "        \"\"\", (datetime.now().isoformat(), account_id))\n",
    "\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        await browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our final function to run the scraper with our inserted information\n",
    "async def run_scraper(id):\n",
    "    # here is where we create our playwright instance and run our scraper (you'll notice previous fuctions have the p parameter)\n",
    "    async with async_playwright() as p:\n",
    "         # Get accounts from the DB\n",
    "        account = get_account(id)\n",
    "        # define the URL we want to scrape\n",
    "        url = \"https://quotes.toscrape.com/login\"\n",
    "\n",
    "        # pass in the parameters to our scrape_page function from the account we retrieved\n",
    "        await scrape_page(\n",
    "            p=p,\n",
    "            url=url,\n",
    "            username=account.get(\"username\"),\n",
    "            password=account.get(\"encrypted_password\"),\n",
    "            proxy=account.get(\"proxy\"),\n",
    "            account_id=account.get(\"id\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping with Proxy: 156.242.45.155:3129\n"
     ]
    }
   ],
   "source": [
    "# run our scraper with the specified account ID!\n",
    "await run_scraper(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
